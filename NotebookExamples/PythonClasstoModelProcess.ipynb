{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Converting Impertaive Python Classes into Models\n",
    "\n",
    "Creating a delcartive and descriptive computing interface that uses existing python libraries to power it requires connecting the imperative python code behind the scenes to the user facing controls. The first step in that process is taking the python classes powering the code and turning them into models - json schemas that can be used for data validation. There are three key pieces to that process: the imperative python dataclass, type hinting, and the `ytBaseModel` class which is based on pydantic's `BaseModel` class. The idea is turn the imperative class into a dataclass that has typed attributes and takes `ytBaseModel` as an argument. Half of that task is easy - pick the class you want to convert, add type hints to the attributes (if there aren't any already) and add `ytBaseModel` as argument. The other half of that task requires some domain knowledge. You have to kmow which attributes are required for the class to work, where the class should fit into the larger model, and where that class lives in the original codebase. Once you figure that, it's easy and fast to add new classes to analysis schema."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Pick a class to convert and identify attributes and variables necessay for it to function\n",
    "\n",
    "For example, in yt we have a class that creates a sphere out of the data, based on coordinates the user provides. In yt, the class looks something like this (I've commmented out code that relies on other classes or functions for demo purposes):\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "class YTSphere():\n",
    "    \"\"\"\n",
    "    A sphere of points defined by a *center* and a *radius*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    center : array_like\n",
    "        The center of the sphere.\n",
    "    radius : float, width specifier, or YTQuantity\n",
    "        The radius of the sphere. If passed a float,\n",
    "        that will be interpreted in code units. Also\n",
    "        accepts a (radius, unit) tuple or YTQuantity\n",
    "        instance with units attached.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> import yt\n",
    "    >>> ds = yt.load(\"RedshiftOutput0005\")\n",
    "    >>> c = [0.5,0.5,0.5]\n",
    "    >>> sphere = ds.sphere(c, (1., \"kpc\"))\n",
    "    \"\"\"\n",
    "    _type_name = \"sphere\"\n",
    "    _con_args = ('center', 'radius')\n",
    "    def __init__(self, center, radius, ds=None,\n",
    "                 field_parameters=None, data_source=None):\n",
    "        # validate_center(center)\n",
    "        # validate_float(radius)\n",
    "        # validate_object(ds, Dataset)\n",
    "        # validate_object(field_parameters, dict)\n",
    "        # validate_object(data_source, YTSelectionContainer)\n",
    "        super(YTSphere, self).__init__(center, ds,\n",
    "                                           field_parameters, data_source)\n",
    "        # Unpack the radius, if necessary\n",
    "        # radius = fix_length(radius, self.ds)\n",
    "        # if radius < self.index.get_smallest_dx():\n",
    "        #     raise YTSphereTooSmall(ds, radius.in_units(\"code_length\"),\n",
    "        #                            self.index.get_smallest_dx().in_units(\"code_length\"))\n",
    "        self.set_field_parameter('radius', radius)\n",
    "        self.set_field_parameter(\"center\", self.center)\n",
    "        self.radius = radius"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create python dataclass with the same name and add attributes with correct type hints\n",
    "\n",
    "Looking at this class, it's clear there are two arguments that are required for this function to work: `center` and `radius`. From the docstring, we can see that the type hint for `center` is array-like (or a list) and the type hint for `radius` is either a float, a special yt data container called `YTQuantity`, or a tuple. It's not clear what the accepted types in the tuple are, so we can look at the examples. It looks like the first type should be a float, and the second type should be a string. It's also unclear what a `YTQuantity` is. Using that information, we can now create a dataclass for sphere, with all the correct parts for it to function. We name the class, add `ytDataObjectAbstract` as arguement, which is a `BaseModel` class meant just for yt data objects (it iterates through the data registry instead of all yt classes), a group that sphere belongs too. For demo purposes, I've added just `BaseModel` so we can see what the model looks like. I add the `center` and `radius` attributes, _keeping the exact same name as the attributes in original class_. This is important as the analysis schema uses the attribute names to match the correct arguments and passes the data on accordingly. The last piece, also critical for analysis schema functionality, is adding the `_yt_operation` name. This is a string that matches the internal name of the imperative class to the internal name of the dataclass. This is how yt knows these two classes - the original class and the dataclass - are really the same thing.\n",
    "\n",
    "Below is what the dataclass version of sphere looks like. I've also printed out the model that is generated from the `BaseModel` argument. From the dataclass a json schema is produced, which provides a form of data validation that controls what kind of data can be entered and then run. This provides the foundation of the declarative user interface. The dataclasses turned model creates a structured guide for users entering data. The model validates the user workflow, but behind the scences the analysis schema is running the imperative code - the original class, with the attributes as the arguments - and producing an output. In this example I used `BaseModel`, but in the analysis schema different versions of `ytBaseModel` are used to both create the model and run the imperative code.\n",
    "\n",
    "The user doesn't see the printed out JSON code below; but they will use it to validate any enter they enter. For example, any time in their workflow that want to create a sphere, the analysis schema will make sure they add a `center` and a `radius` in the correct format. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union, Tuple, Optional\n",
    "\n",
    "class Sphere(BaseModel):\n",
    "    \"\"\"A sphere of points defined by a *center* and a *radius*.\n",
    "    \"\"\"\n",
    "    center: List[float] = Field(alias=\"Center\")\n",
    "    radius: Union[float, Tuple[float, str]] = Field(alias=\"Radius\")\n",
    "    _yt_operation: str = \"sphere\"\n",
    "\n",
    "print(Sphere.schema_json(indent=2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "  \"title\": \"Sphere\",\n",
      "  \"description\": \"A sphere of points defined by a *center* and a *radius*.\\n    \",\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "    \"Center\": {\n",
      "      \"title\": \"Center\",\n",
      "      \"type\": \"array\",\n",
      "      \"items\": {\n",
      "        \"type\": \"number\"\n",
      "      }\n",
      "    },\n",
      "    \"Radius\": {\n",
      "      \"title\": \"Radius\",\n",
      "      \"anyOf\": [\n",
      "        {\n",
      "          \"type\": \"number\"\n",
      "        },\n",
      "        {\n",
      "          \"type\": \"array\",\n",
      "          \"items\": [\n",
      "            {\n",
      "              \"type\": \"number\"\n",
      "            },\n",
      "            {\n",
      "              \"type\": \"string\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"Center\",\n",
      "    \"Radius\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding the right class\n",
    "It is important to add the correct BaseModel class as an argument to the data class. There are different classes designed for different functionality - including ytBaseModel and ytDataObjectAbstract. Both of these classes inherit from pydantic's `BaseModel` and have a function call `_run` that cycle through yt and find the correct functions to run. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "Once there is dataclass has become model, it can be integrated into the larger analysis schema model by adding the dataclass reference (the name) into the model, either by adding it as an attribute or hint to another model, or adding as a type hint to the main model. In the example below, `Sphere` is added as part of the `data_source` attribute. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class SlicePlot(BaseModel):\n",
    "    # ds: Optional[Dataset] = Field(alias=\"Dataset\")\n",
    "    # fields: FieldNames = Field(alias=\"FieldNames\")\n",
    "    # axis: str = Field(alias=\"Axis\")\n",
    "    # center: Optional[Union[str, List[float]]] = Field(alias=\"Center\")\n",
    "    # width: Optional[Union[List[str], Tuple[int, str]]] = Field(alias=\"Width\")\n",
    "    data_source: Optional[Sphere]\n",
    "    # Comments: Optional[str]\n",
    "    # _yt_operation: str = \"SlicePlot\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You're converted a python class into a analysis schema model! To test it, follow these steps:\n",
    "\n",
    "4. Run the model to create the schema file, it should appear in the definitions section and correct reference wherever it was placed\n",
    "5. Enter data into a JSON file using the schema for validation. The class and it's attributes should be available to enter data in\n",
    "6. Run the JSON file. The ouptut should have run data you entered into that classes and should have applied it to the output."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('analysis-schema')"
  },
  "interpreter": {
   "hash": "7b8481f636dd103fe4e024ad93fbbcf32ef04ed0811f2c8eb9bedd37cdc7e808"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}